# script_build_api_2560.py
import os
import json
import time
import re
import argparse
import asyncio
import xxhash
import numpy as np
import pdfplumber
import torch
from openai import AsyncOpenAI
from sentence_transformers import SentenceTransformer
from graphr1 import GraphR1
from graphr1.utils import wrap_embedding_func_with_attrs


# ---------------------------------------------
# 1. é…ç½®è·¯å¾„ä¸ API
# ---------------------------------------------
# LLM é…ç½® (DeepSeek API)
API_KEY = "sk-45a3b1bbcdc34df2a9805b7614ac951f" 
BASE_URL = "https://api.deepseek.com"
MODEL_NAME = "deepseek-chat"

# Embedding é…ç½® (æœ¬åœ° Qwen æ¨¡å‹)
EMBED_MODEL_PATH = "/root/Qwen3-Embedding-4B"

# æ•°æ®ç›®å½• (ä¿æŒæ‚¨è„šæœ¬ä¸­çš„è·¯å¾„)
DATA_DIR = "/root/Graph-R1/data_for_hypergraph"

# ---------------------------------------------
# 2. åˆå§‹åŒ–æ¨¡å‹
# ---------------------------------------------

# A. åˆå§‹åŒ– DeepSeek API å®¢æˆ·ç«¯
client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
print(f"LLM å®¢æˆ·ç«¯å·²å°±ç»ª: {MODEL_NAME}")

# B. åˆå§‹åŒ–æœ¬åœ° Qwen Embedding æ¨¡å‹
print(f"æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹: {EMBED_MODEL_PATH} ...")
try:
    # å°è¯•å¼€å¯ flash_attention_2 ä»¥åŠ é€Ÿ (å¦‚æœæ˜¾å¡æ”¯æŒ)
    embed_model = SentenceTransformer(
        EMBED_MODEL_PATH,
        trust_remote_code=True,
        model_kwargs={"attn_implementation": "flash_attention_2", "device_map": "auto"},
        tokenizer_kwargs={"padding_side": "left"}
    )
except Exception as e:
    print(f"Flash Attention åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æ¨¡å¼: {e}")
    embed_model = SentenceTransformer(
        EMBED_MODEL_PATH, 
        trust_remote_code=True,
        device="cuda:1" # ä¿®æ”¹ä¸ºCPU ID ä¸º 1 çš„ GPU
    )

# åŠ¨æ€è·å–æ¨¡å‹ç»´åº¦ (ç¡®ä¿ç»´åº¦å‚æ•°ç»å¯¹æ­£ç¡®)
EMBEDDING_DIM = embed_model.get_sentence_embedding_dimension()
print(f"Embedding æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œç»´åº¦: {EMBEDDING_DIM}")

# ---------------------------------------------
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ---------------------------------------------

# --- æ–°å¢: è‡ªå®šä¹‰ Embedding å‡½æ•° (é€‚é… GraphR1 æ¥å£) ---
@wrap_embedding_func_with_attrs(embedding_dim=EMBEDDING_DIM, max_token_size=8192)
async def my_qwen_embedding(texts: list[str], **kwargs) -> np.ndarray:
    """
    ä½¿ç”¨ Qwen-Embedding-4B ç”Ÿæˆå‘é‡ã€‚
    """
    # ä½¿ç”¨ asyncio.to_thread å°†åŒæ­¥çš„ GPU è®¡ç®—æ”¾å…¥çº¿ç¨‹æ± ï¼Œé˜²æ­¢é˜»å¡äº‹ä»¶å¾ªç¯
    embeddings = await asyncio.to_thread(
        embed_model.encode, 
        texts, 
        convert_to_numpy=True, 
        show_progress_bar=False,
        batch_size=16 # æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´
    )
    return embeddings

# --- åŸæœ‰: API è°ƒç”¨åŒ…è£…å™¨ ---
async def my_api_llm_call(prompt: str, system_prompt: str = None, history_messages: list = [], **kwargs) -> str:
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})

    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            stream=False,
            temperature=0.0,
            max_tokens=4096
        )
        content = response.choices[0].message.content
        return content if content else ""
    except Exception as e:
        print(f"API è°ƒç”¨å¤±è´¥: {e}")
        return ""

# --- åŸæœ‰: é«˜è´¨é‡ PDF è§£æå™¨ ---
def parse_pdf_high_quality(file_path):
    full_text = []
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                # 1. å°è¯•æå–è¡¨æ ¼
                tables = page.extract_tables()
                table_texts = []
                for table in tables:
                    cleaned_table = [[cell if cell else "" for cell in row] for row in table]
                    if cleaned_table:
                        header = " | ".join(cleaned_table[0])
                        separator = " | ".join(["---"] * len(cleaned_table[0]))
                        body = "\n".join([" | ".join(row) for row in cleaned_table[1:]])
                        table_texts.append(f"\n{header}\n{separator}\n{body}\n")

                # 2. æå–æ­£æ–‡
                text = page.extract_text(x_tolerance=2, y_tolerance=3)
                
                if text: full_text.append(text)
                if table_texts: full_text.extend(table_texts)
                    
    except Exception as e:
        print(f"PDF è§£æå¤±è´¥ {file_path}: {e}")
        return ""
    return "\n".join(full_text)

# --- æ–°å¢ï¼šè¯»å–markdownæ–‡ä»¶å†…å®¹çš„å‡½æ•° ---
# è¯»å– data_for_hypergraph/NCCN/æ–‡çŒ®æ–‡ä»¶å¤¹/ â†’ æŒ‡å—å = "NCCN"ï¼Œ
# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–è¯»å–ç¬¬ä¸€ä¸ª#æ ‡è®°å†…å®¹ä½œä¸ºæ ‡é¢˜
# è¯»å–åŒæ–‡ä»¶å¤¹ä¸‹çš„PMID.txtä½œä¸ºPMID
def load_paper_from_folder(paper_dir: str) -> dict:
    """
    ä»å•ä¸ªæ–‡çŒ®æ–‡ä»¶å¤¹ä¸­è¯»å– Paper å…ƒæ•°æ®ï¼š
    - guidelineï¼ˆæŒ‡å—åï¼‰
    - titleï¼ˆMarkdown ç¬¬ä¸€ä¸ª # æ ‡é¢˜ï¼‰
    - pmidï¼ˆPMID.txtï¼‰
    - contentï¼ˆfull.md å…¨æ–‡ï¼‰
    - /root/Graph-R1/data_for_hypergraph/NCCN/paper_001
    -/root/Graph-R1/data_for_hypergraph/FIGO/paper_001
    """

    # ---------- 1. åŸºæœ¬è·¯å¾„æ ¡éªŒ ----------
    if not os.path.isdir(paper_dir):
        raise ValueError(f"ä¸æ˜¯æœ‰æ•ˆçš„æ–‡çŒ®æ–‡ä»¶å¤¹: {paper_dir}")

    md_path = os.path.join(paper_dir, "full.md")
    pmid_path = os.path.join(paper_dir, "PMID.txt")

    if not os.path.exists(md_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ° full.md: {md_path}")

    # ---------- 2. guideline = ä¸Šä¸€çº§ç›®å½•å ----------
    # data_for_hypergraph/NCCN/xxx_paper_folder/
    guideline = os.path.basename(os.path.dirname(paper_dir))

    # ---------- 3. è¯»å– Markdown å…¨æ–‡ ----------
    with open(md_path, "r", encoding="utf-8") as f:
        markdown_text = f.read().strip()

    # ---------- 4. ä½¿ç”¨æ­£åˆ™æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ª # å¼€å¤´çš„è¡Œï¼‰ ----------
    title = None
    for line in markdown_text.splitlines():
        line = line.strip()
        if line.startswith("#"):
            # å»æ‰æ‰€æœ‰å‰å¯¼ # å’Œç©ºæ ¼
            title = re.sub(r"^#+\s*", "", line)
            break

    if not title:
        raise ValueError(f"æœªåœ¨ Markdown ä¸­æ‰¾åˆ°æ ‡é¢˜ (#): {md_path}")

    # ---------- 5. è¯»å– PMID ----------
    # å¦‚æœ PMID.txt å­˜åœ¨ä¸”éç©ºï¼Œåˆ™è¯»å–å…¶å†…å®¹
    # å¦‚æœPMIDä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™ç”¨æ ‡é¢˜çš„Hashå€¼ä»£æ›¿
    pmid = None
    if os.path.isfile(pmid_path):
        with open(pmid_path, 'r', encoding='utf-8') as f:
            pmid = f.read().strip() or None
    if pmid and not pmid.isdigit():  # éæ³• PMID å›é€€
        pmid = None
    if not pmid:  # ç¡®å®šæ€§å“ˆå¸Œ
        pmid = str(xxhash.xxh64(title.encode('utf-8')).intdigest())


    # ---------- 6. è¿”å›ç»Ÿä¸€ç»“æ„ ----------
    return {
        "guideline": guideline,     # æŒ‡å—
        "title": title,             # æ ‡é¢˜
        "pmid": pmid,               # PMID
        "content": markdown_text,   # å…¨æ–‡å†…å®¹
    }

# ---------------------------------------------
# 4. ä¸»æ„å»ºé€»è¾‘
# ---------------------------------------------
async def extract_knowledge(rag, paper_content, paper_name):
    """
       é’ˆå¯¹ã€å•ç¯‡ Paperã€‘è¿›è¡ŒçŸ¥è¯†æŠ½å–
       """
    print(f"å¼€å§‹æ’å…¥æ–‡æ¡£{paper_name}çš„ç›¸å…³èŠ‚ç‚¹")

    max_retries = 5 # æœ€å¤§é‡è¯•æ¬¡æ•°
    for attempt in range(1, max_retries + 1):
        try:
            await rag.ainsert(
                paper_content,
                paper_name=paper_name
            )
            print(f"âœ… æ–‡çŒ® {paper_name} æŠ½å–å®Œæˆ")
            return

        except Exception as e:
            print(f"âš ï¸ æ–‡çŒ® {paper_name} ç¬¬ {attempt}/{max_retries} æ¬¡å¤±è´¥: {e}")
            if attempt == max_retries:
                raise
            await asyncio.sleep(5) # ç­‰å¾…åé‡è¯•

    # batch_size = 50
    # total_batches = (len(unique_contexts) + batch_size - 1) // batch_size
    #
    # for i in range(0, len(unique_contexts), batch_size):
    #     batch_contexts = unique_contexts[i:i + batch_size]
    #     print(f"--- æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {(i // batch_size) + 1}/{total_batches} ---")
    #
    #     retries = 0
    #     while retries < 5:
    #         try:
    #             await rag.ainsert(batch_contexts)
    #             print(f"æ‰¹æ¬¡ {(i // batch_size) + 1} æˆåŠŸæ’å…¥ã€‚")
    #             break
    #         except Exception as e:
    #             retries += 1
    #             print(f"é‡è¯• {retries}/5: {e}")
    #             await asyncio.sleep(5)

async def insert_knowledge(rag, paper_content, paper_name):

    # å˜åŒ–ï¼šrag åˆå§‹åŒ– è½¬ç§»åˆ°äº† main å‡½æ•°ä¸­ï¼Œå› ä¸ºè¦æ ¹æ®PMIDå»é‡ï¼Œéœ€è¦è®¿é—® rag å®ä¾‹æŸ¥è¯¢
    # rag = GraphR1(
    #     working_dir=f"expr/{data_source}",
    #
    #     # LLM éƒ¨åˆ†
    #     llm_model_func=my_api_llm_call,
    #     llm_model_name=MODEL_NAME,
    #
    #     # --- æ–°å¢: Embedding éƒ¨åˆ† ---
    #     embedding_func=my_qwen_embedding,
    #
    #     # --- æ–°å¢: ç»´åº¦åŒæ­¥ ---
    #     # å¿…é¡»ç¡®ä¿å›¾åµŒå…¥(Node2Vec)çš„ç»´åº¦ä¸æ–‡æœ¬åµŒå…¥ç»´åº¦ä¸€è‡´
    #     node2vec_params={
    #         "dimensions": EMBEDDING_DIM,
    #         "num_walks": 10,
    #         "walk_length": 40,
    #         "window_size": 2,
    #         "iterations": 3,
    #         "random_seed": 3,
    #     },
    #
    #     # å…¶ä»–é…ç½®
    #     chunk_token_size=1600,
    #     chunk_overlap_token_size=50,
    #     graph_storage="Neo4JStorage"
    # )
    await extract_knowledge(rag, paper_content, paper_name)
    print(f"çŸ¥è¯†è¶…å›¾ä¸º '{data_source}' æ„å»ºæˆåŠŸã€‚")


# ---------------------------------------------
# 5. ä¸»å¼‚æ­¥å‡½æ•°
# ---------------------------------------------
async def main():
    # ==================================
    parser = argparse.ArgumentParser()
    # å»ºè®®æ›´æ”¹ data_source åç§°ä»¥é¿å…ä¸æ—§çš„ 1536 ç»´æ•°æ®å†²çª
    parser.add_argument("--data_source", type=str, default="DeepSeek_QwenEmbed_Graph")
    args = parser.parse_args()
    # ===================================


    # --- æ–°å¢: åˆå§‹åŒ– GraphR1 å®ä¾‹ ---
    rag = GraphR1(
        working_dir=f"expr/{args.data_source}",

        # LLM éƒ¨åˆ†
        llm_model_func=my_api_llm_call,
        llm_model_name=MODEL_NAME,

        # --- æ–°å¢: Embedding éƒ¨åˆ† ---
        embedding_func=my_qwen_embedding,

        # --- æ–°å¢: ç»´åº¦åŒæ­¥ ---
        # å¿…é¡»ç¡®ä¿å›¾åµŒå…¥(Node2Vec)çš„ç»´åº¦ä¸æ–‡æœ¬åµŒå…¥ç»´åº¦ä¸€è‡´
        node2vec_params={
            "dimensions": EMBEDDING_DIM,
            "num_walks": 10,
            "walk_length": 40,
            "window_size": 2,
            "iterations": 3,
            "random_seed": 3,
        },

        # å…¶ä»–é…ç½®
        chunk_token_size=1600,
        chunk_overlap_token_size=50,
        graph_storage="Neo4JStorage"
    )
    print("âœ… GraphR1åˆå§‹åŒ–å®Œæˆ")

    # ç»Ÿè®¡å˜é‡
    stats = {
        "total": 0,
        "new": 0,
        "existing": 0,
        "errors": 0
    }


    print(f"å¼€å§‹ä» {DATA_DIR} åŠ è½½æ•°æ®...")

    unique_contexts = []

    if not os.path.exists(DATA_DIR):
        print(f"é”™è¯¯ï¼šæ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
        exit(1)

    # éå†ç›®å½•ç»“æ„: data_for_hypergraph/{GuidelineName}/{PaperFolder}/full.md
    for guideline_name in os.listdir(DATA_DIR):
        guideline_path = os.path.join(DATA_DIR, guideline_name)
        if not os.path.isdir(guideline_path):
            continue

        print(f"\nğŸ“ å¤„ç†æŒ‡å—: {guideline_name}")

        for paper_folder in os.listdir(guideline_path):
            paper_path = os.path.join(guideline_path, paper_folder)
            if not os.path.isdir(paper_path):
                continue

            stats["total"] += 1 # ç»Ÿè®¡æ€»æ–‡çŒ®æ•°

            try:
                # 1. è·å– paper å…ƒæ•°æ®
                paper = load_paper_from_folder(paper_path)  # è¿”å› dict å¾—åˆ° paper å…ƒæ•°æ®

                paper_pmid = paper["pmid"]  # è·å–pmid
                paper_title = paper["title"]  # è·å–æ ‡é¢˜
                paper_guideline = paper["guideline"]  # è·å–æŒ‡å—å
                paper_content = paper["content"]  # è·å–å…¨æ–‡å†…å®¹

                # 2. å»é‡æ£€æŸ¥ï¼šä¼˜å…ˆé€šè¿‡ PMID æŸ¥æ‰¾åº“é‡Œæ˜¯å¦å·²ç»æœ‰è¿™ä¸ª Paper èŠ‚ç‚¹
                existing_name = await rag.chunk_entity_relation_graph.get_paper_by_pmid(paper_pmid) if paper_pmid else None

                # å¦‚æœå­˜åœ¨ï¼Œåˆ™æ›´æ–°å…¶æ‰€å±æŒ‡å—åˆ—è¡¨å¹¶è·³è¿‡ LLM æå–
                if existing_name:
                    print(f"æ–‡çŒ® {paper_pmid} å·²å­˜åœ¨ï¼Œæ›´æ–°æ‰€å±æŒ‡å—: {paper_guideline}")
                    await rag.chunk_entity_relation_graph.update_paper_guidelines(existing_name, paper_guideline) # æ›´æ–°æŒ‡å—åˆ—è¡¨
                    stats["existing"] += 1
                    continue  # è·³è¿‡ LLM æå–ï¼Œå¤„ç†ä¸‹ä¸€ç¯‡

                # 3. å¦‚æœä¸å­˜åœ¨ï¼Œä¸ºæ–°æ–‡çŒ®ï¼Œåˆ™æ–°å»ºæ–‡çŒ®èŠ‚ç‚¹paper
                stats["new"] += 1
                paper_node_id = f"paper::{paper_pmid}" # å”¯ä¸€èŠ‚ç‚¹å
                print(f"  æ­£åœ¨åŠ è½½æ–°æ–‡çŒ®: {paper['title']} | PMID: {paper['pmid']} | æ‰€å±æŒ‡å—:{paper['guideline']} ")

                # åˆ›å»º paper èŠ‚ç‚¹
                await rag.chunk_entity_relation_graph.upsert_node(
                    node_name=paper_node_id,
                    node_data=
                        {
                        "role": "paper",                    # èŠ‚ç‚¹è§’è‰²ä¸ºpaper
                        "pmid": paper_pmid,
                        "guidelines": [paper_guideline],
                        "title": paper_title,
                        }
                )

                # 4. è¯»å–å…¨æ–‡å†…å®¹ï¼Œæ’å…¥çŸ¥è¯†åº“
                content = paper_content
                paper_name = f"paper::{paper_pmid}" # ç»Ÿä¸€çš„ paper èŠ‚ç‚¹å

                # ä¼ å…¥ paper_nameï¼Œè®©åç»­ç”Ÿæˆçš„è¶…è¾¹è‡ªåŠ¨å…³è”åˆ°å®ƒï¼Œä½¿ç”¨ä¹‹å‰çš„æ¥å£ï¼Œä½†æ˜¯æ–°å¢ä¸€ä¸ªpaper_nameå‚æ•°å’Œragå®ä¾‹
                # é€»è¾‘ç”±å¤šä¸ªæ–‡ä»¶çš„unique_contextså˜æˆå•ä¸ªæ–‡ä»¶çš„contentï¼Œå› ä¸ºè¦è¿½æº¯åˆ°paperèŠ‚ç‚¹
                try:
                    await insert_knowledge(rag, content, paper_name)
                except Exception as e:
                    print(f"  æ’å…¥çŸ¥è¯†åº“å¤±è´¥ {paper_path} : {e}")
                    stats["errors"] += 1

            except Exception as e:
                print(f"  åŠ è½½å¤±è´¥ {paper_path} : {e}")

    print("\n--- æ–‡çŒ®å¤„ç†ç»Ÿè®¡ ---")
    print(f"æ€»æ–‡çŒ®æ•°: {stats['total']}")
    print(f"æ–°æ–‡çŒ®æ•°: {stats['new']}")
    print(f"å·²å­˜åœ¨æ–‡çŒ®æ•°: {stats['existing']}")
    print(f"å¤„ç†é”™è¯¯æ•°: {stats['errors']}")


    # if not unique_contexts:
    #     print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆ Markdown æ–‡çŒ®ã€‚")
    #     exit(1)
    # print(f"æˆåŠŸåŠ è½½äº† {len(unique_contexts)} ç¯‡æ–‡çŒ®ã€‚")



    # ---- åŸæœ‰çš„æ–‡ä»¶éå†é€»è¾‘ (å·²æ³¨é‡Š) ----
    # for filename in os.listdir(DATA_DIR):
    #     file_path = os.path.join(DATA_DIR, filename)
    #     try:
    #         if filename.endswith(".txt"):
    #             print(f"  æ­£åœ¨åŠ è½½ (TXT): {filename}")
    #             with open(file_path, 'r', encoding='utf-8') as f:
    #                 unique_contexts.append(f.read())
    #         elif filename.endswith(".jsonl"):
    #             print(f"  æ­£åœ¨åŠ è½½ (JSONL): {filename}")
    #             with open(file_path, 'r', encoding='utf-8') as f:
    #                 for line in f:
    #                     data = json.loads(line)
    #                     if "contents" in data: unique_contexts.append(data["contents"])
    #                     elif "text" in data: unique_contexts.append(data["text"])
    #         elif filename.endswith(".pdf"):
    #             print(f"  æ­£åœ¨åŠ è½½ (PDF): {filename}")
    #             content = parse_pdf_high_quality(file_path)
    #             if len(content) > 50:
    #                 unique_contexts.append(content)
    #             else:
    #                 print(f"  è­¦å‘Š: PDF {filename} å†…å®¹è¿‡çŸ­")
    #         else:
    #             print(f"  è·³è¿‡: {filename}")
    #     except Exception as e:
    #         print(f"è¯»å–æ–‡ä»¶ {filename} å‡ºé”™: {e}")
    # if not unique_contexts:
    #     print("é”™è¯¯ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ã€‚")
    #     exit(1)
    # print(f"æˆåŠŸåŠ è½½äº† {len(unique_contexts)} ä¸ªæ–‡æ¡£ã€‚")

    # try:
    #     asyncio.run(insert_knowledge(args.data_source, unique_contexts))
    # except Exception as e:
    #     print(f"æ„å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
# ---------------------------------------------
# 6. ç¨‹åºå…¥å£
# ---------------------------------------------
if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())


